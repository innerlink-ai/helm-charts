# Global settings
namespace: innerlink


registry:
  #For airgapped deployments
  base_url: "localhost:5000"
  busy_box_url: "localhost:5000/ghcr.io/containerd"
  postgres_url: "localhost:5000"
  url: "localhost:5000/ghcr.io/innerlink-ai"
  huggingface_url: "localhost:5000/ghcr.io/huggingface"

istio:
  enabled: true
  gateway:
    name: innerlink-gw
    port: 8080          # keep 8080 (already works) or change to 80 + hostNetwork
  hosts:
    - "*"               # wildcard is OK when AuthZ is in place
  mTlsMode: STRICT      # meshâ€‘internal
  authzAllowAll: true   # initial open policy; tighten later


tgi:
  args:
    maxBatchPrefillTokens: 1024
  model:
    #id: "meta-llama/Llama-3.1-8B-Instruct"
    path: "/data/meta-llama/Llama-3.1-8B-Instruct"
    #path: "/data/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590"
    offline: "1"
  gpu:
    tensorParallelSize: "1"
    cudaVisibleDevices: "0"

# PostgreSQL Configuration
postgres:
  ssl:
    # Certificate will be read directly from local filesystem
    certPath: "/etc/postgresql/ssl/postgres-certs"
backend:
  enabled: true
frontend:
  enabled: true  