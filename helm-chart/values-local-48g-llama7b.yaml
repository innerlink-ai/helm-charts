# Global settings
namespace: innerlink


registry:
  #For airgapped deployments
  base_url: "localhost:5000"
  url: "localhost:5000/ghcr.io/innerlink-ai"
  huggingface_url: "localhost:5000/ghcr.io/huggingface"

istio:
  enabled: true
  gateway:
    name: innerlink-gw
    port: 8080          # keep 8080 (already works) or change to 80 + hostNetwork
  hosts:
    - "*"               # wildcard is OK when AuthZ is in place
  mTlsMode: STRICT      # meshâ€‘internal
  authzAllowAll: true   # initial open policy; tighten later



tgi:
  args:
    maxBatchPrefillTokens: 4096
  model:
    id: "meta-llama/Llama-2-7b-chat-hf"
    path: "/data/model_weights/meta-llama/Llama-2-7b-chat-hf"
    offline: "1"
  gpu:
    tensorParallelSize: "1"
    cudaVisibleDevices: "0"

# PostgreSQL Configuration
postgres:
  ssl:
    # Certificate will be read directly from local filesystem
    certPath: "/etc/postgresql/ssl/postgres-certs"
