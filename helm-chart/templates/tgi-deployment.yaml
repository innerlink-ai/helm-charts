apiVersion: apps/v1
kind: Deployment
metadata:
  name: tgi
  namespace: {{ .Values.namespace }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tgi
  template:
    metadata:
      labels:
        app: tgi
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      runtimeClassName: nvidia
      containers:
      - name: tgi
        image: "{{ .Values.registry.huggingface_url }}/text-generation-inference:1.4.3"
        args:
        - "--max-batch-prefill-tokens"
        - "{{ .Values.tgi.args.maxBatchPrefillTokens }}"
        #resources:
          #limits:
            #nvidia.com/gpu: 1
        ports:
        - containerPort: 8080
        env:
        - name: MODEL_ID
          value: "meta-llama/Llama-2-7b-chat-hf"
        - name: TENSOR_PARALLEL_SIZE
          value: "4"
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"
        - name: HOST
          value: "0.0.0.0"
        - name: PORT
          value: "8080"
        - name: HUGGING_FACE_HUB_CACHE
          value: "/data"
        - name: DTYPE
          value: "float16"
        - name: CUDA_MEMORY_FRACTION
          value: "0.75"
        - name: NUMBA_CACHE_DIR # Add this line
          value: "/data/.cache/numba"
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
          readOnlyRootFilesystem: false
        volumeMounts:
        - name: model-weights-storage
          mountPath: /data
      volumes:
      - name: model-weights-storage
        persistentVolumeClaim:
          claimName: model-weights-pvc
