apiVersion: apps/v1
kind: Deployment
metadata:
  name: tgi
  namespace: {{ .Values.namespace }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tgi
  template:
    metadata:
      labels:
        app: tgi
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      runtimeClassName: nvidia
      containers:
      - name: tgi
        image: "localhost:5000/innerlink-tgi-with-llama:2.4.1"
        args:
        - "--max-batch-prefill-tokens"
        - "{{ .Values.tgi.args.maxBatchPrefillTokens }}"
        ports:
        - containerPort: 8080
        env:
        - name: MODEL_ID
          value: "/data"
        - name: HF_HUB_OFFLINE
          value: "{{ .Values.tgi.model.offline }}"
        - name: TENSOR_PARALLEL_SIZE
          value: "{{ .Values.tgi.gpu.tensorParallelSize }}"
        - name: CUDA_VISIBLE_DEVICES
          value: "{{ .Values.tgi.gpu.cudaVisibleDevices }}"
        - name: HOST
          value: "0.0.0.0"
        - name: PORT
          value: "8080"
        - name: DTYPE
          value: "float16"
        - name: NUMBA_CACHE_DIR
          value: "/tmp/.cache/numba"
        - name: OUTLINES_CACHE_DIR
          value: "/tmp/.cache/outlines"
        - name: TRANSFORMERS_CACHE
          value: "/tmp/.cache/huggingface"
        - name: TRITON_CACHE_DIR
          value: "/tmp/.cache/triton"
        - name: XDG_CACHE_HOME
          value: "/tmp/.cache"
        - name: MAX_TOTAL_TOKENS
          value: "2056"
        - name: MAX_INPUT_LENGTH
          value: "1024"
        - name: MAX_BATCH_PREFILL_TOKENS
          value: "2056"
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
          readOnlyRootFilesystem: false
        volumeMounts:
        - name: shm
          mountPath: /dev/shm
      volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 1Gi